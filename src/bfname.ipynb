{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Function Name Recovery\n",
    "\n",
    "This notebook contains experiments for binary function name recovery.\n",
    "\n",
    "## 1. Load Dataset\n",
    "\n",
    "We first load the test datasets. Here, `test_dataset` is for decompiled-only and retrieval-augmented inputs, while `probed_test_dataset` is for ProRec inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# load dataset\n",
    "CACHE_DIR = '../save/.cache'\n",
    "OUTPUT_DIR ='../save/binsum'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    'PurCL/lmpa-prorec-test-1k',\n",
    "    cache_dir=CACHE_DIR\n",
    ")['test']\n",
    "probed_test_dataset = load_dataset(\n",
    "    'PurCL/lmpa-prorec-test-probed-c34b-1k',\n",
    "    cache_dir=CACHE_DIR\n",
    ")['test']\n",
    "print(test_dataset)\n",
    "print(probed_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datastore\n",
    "\n",
    "The retrieval results are pre-computed and stored in the `test_dataset` in the form of datastore source function ids. Therefore, we need to access the datastore to map those ids to real functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from evaluator.binsum_evaluator import tokenizer\n",
    "from tools.retriever import CrossModalRetriever\n",
    "\n",
    "hf_hub_download(\n",
    "    repo_id='PurCL/casp-moco-lmpa-c-only',\n",
    "    subfolder='index',\n",
    "    filename='keys.pkl',\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "hf_hub_download(\n",
    "    repo_id='PurCL/casp-moco-lmpa-c-only',\n",
    "    subfolder='index',\n",
    "    filename='key_embeddings.npy',\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "index_path = '../save/.cache/models--PurCL--casp-moco-lmpa-c-only/snapshots/472d22fe7590403f9078799c6b83ff645eff9534/index'\n",
    "\n",
    "retriever = CrossModalRetriever()   # without model\n",
    "retriever.load_index(index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing\n",
    "\n",
    "We randomly sample 100 examples for reproduction. The pre-processing includes normalizing strings in the decompiled code and constructing inputs for three methods to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# sampled_indexes = random.sample(range(len(test_dataset)), 10)\n",
    "sampled_indexes = random.sample(range(len(test_dataset)), 100)\n",
    "\n",
    "\n",
    "string_pattern = r'([\\'\"])(?:(?=(\\\\?))\\2.)*?\\1'\n",
    "def normalize_string(raw_decompiled_function):\n",
    "    normalized_decompiled_function = re.sub(string_pattern, '<some_string>', raw_decompiled_function)\n",
    "    return normalized_decompiled_function\n",
    "\n",
    "\n",
    "def extract_probed(probed_sources):\n",
    "    strip_len = len('<asm_token>\\n')\n",
    "    processed_probed_sources = []\n",
    "    for ps in probed_sources:\n",
    "        asm_idx = ps.find('<asm_token>')\n",
    "        if asm_idx == -1:\n",
    "            processed_probed_sources.append(ps)\n",
    "        else:\n",
    "            pps = ps[asm_idx + strip_len:]\n",
    "            processed_probed_sources.append(pps)\n",
    "    return processed_probed_sources\n",
    "\n",
    "# TOP_K = 3\n",
    "# TOP_K = 4\n",
    "TOP_K = 5\n",
    "\n",
    "batch_lmpa = []\n",
    "batch_dec = []\n",
    "batch_dec_w_pro = []\n",
    "batch_src = []\n",
    "batch_dec_w_ret = []\n",
    "for ex_q, ex_p in zip(test_dataset.select(sampled_indexes), \\\n",
    "                probed_test_dataset.select(sampled_indexes)):\n",
    "    assert ex_q['src'] == ex_p['oracle_source']\n",
    "    batch_lmpa.append(ex_q['lmpa'])\n",
    "    decompiled_function = eval(ex_q['lmpa'])['query'].split('Q:[')[0].strip()\n",
    "    decompiled_function = normalize_string(decompiled_function)\n",
    "    batch_dec.append(decompiled_function)\n",
    "    batch_dec_w_pro.append(\n",
    "        (decompiled_function, extract_probed(ex_p['probed_sources'])[:TOP_K])\n",
    "    )\n",
    "    batch_src.append(ex_p['oracle_source'])\n",
    "    retrieved_sources = []\n",
    "    for score, idx in zip(ex_q['retrieved_index'][0], ex_q['retrieved_index'][1]):\n",
    "        ret_src = retriever.index['keys'][int(idx)]\n",
    "        retrieved_sources.append(tokenizer.decode(tokenizer.encode(ret_src)[:512]))\n",
    "    batch_dec_w_ret.append(\n",
    "        (decompiled_function, retrieved_sources[:TOP_K])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt GPT3.5\n",
    "\n",
    "First, construct prompter. Provide your own OpenAI token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.prompter import (\n",
    "    OpenAIDecompFuncNamer \n",
    ")\n",
    "\n",
    "# key\n",
    "secret_key = 'YOUR_OPENAI_KEY'\n",
    "\n",
    "MODEL_NAME = 'gpt-3.5-turbo-1106'\n",
    "MODEL_NAME_SHORT = 'gpt3.5'\n",
    "\n",
    "dec_func_namer = OpenAIDecompFuncNamer(\n",
    "    secret_key,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "generation_config = {'temperature': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate Function Name with Only Decompiled Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decname_results = []\n",
    "for inputs in batch_dec:\n",
    "    decname_results.append(dec_func_namer.generate(inputs, **generation_config))\n",
    "with open(os.path.join(OUTPUT_DIR, f'decsum_{MODEL_NAME_SHORT}_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(decname_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Function Name with Retrieval-based Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragname_results = []\n",
    "for inputs in batch_dec_w_ret:\n",
    "    ragname_results.append(dec_func_namer.generate_with_augmentation(inputs, **generation_config))\n",
    "with open(os.path.join(OUTPUT_DIR, f'ragsum_{MODEL_NAME_SHORT}_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(ragname_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generate Function Name with ProRec Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parname_results = []\n",
    "for inputs in batch_dec_w_pro:\n",
    "    parname_results.append(dec_func_namer.generate_with_augmentation(inputs, **generation_config))\n",
    "with open(os.path.join(OUTPUT_DIR, f'parsum_{MODEL_NAME_SHORT}_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(parname_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate with SymLM Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluator.bfname_evaluator\n",
    "reload(evaluator.bfname_evaluator)\n",
    "from evaluator.bfname_evaluator import (\n",
    "    camel_to_snake,\n",
    "    extract_function_name,\n",
    "    evaluate_func_names\n",
    ")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, f'aggregated_{MODEL_NAME_SHORT}_results.json'), 'w') as f:\n",
    "    agg_results = []\n",
    "    for decname_res, parname_res, ragname_res, lmpa_data, src, dw_pro, dw_ret in \\\n",
    "        zip(decname_results, parname_results, ragname_results, batch_lmpa, batch_src, batch_dec_w_pro, batch_dec_w_ret):\n",
    "        decname = camel_to_snake(extract_function_name(decname_res.choices[0].message.content))\n",
    "        if decname is None:\n",
    "            print(decname_res.choices[0].message.content)\n",
    "            assert 0\n",
    "        parname = camel_to_snake(extract_function_name(parname_res.choices[0].message.content))\n",
    "        ragname = camel_to_snake(extract_function_name(ragname_res.choices[0].message.content))\n",
    "\n",
    "        agg_results.append(\n",
    "            {\n",
    "                'lmpa': lmpa_data,\n",
    "                'src': src,\n",
    "                'dec': dw_pro[0],\n",
    "                'decname': decname,\n",
    "                'parname': parname,\n",
    "                'probed_sources': dw_pro[1],\n",
    "                'ragname': ragname,\n",
    "                'retrieved_sources': dw_ret[1],\n",
    "            }\n",
    "        )\n",
    "    print(len(agg_results))\n",
    "    json.dump(agg_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator.bfname_evaluator import (\n",
    "    extract_function_name,\n",
    "    evaluate_func_names,\n",
    ")\n",
    "\n",
    "def parse_src_func_name(src_func):\n",
    "    src_func_name = re.split(' |\\n|\\t', src_func.split('(')[0])\n",
    "    if src_func_name[-1] == '':  # ... Member (long AllocSize)\n",
    "        src_func_name = src_func_name[-2]\n",
    "    else:\n",
    "        src_func_name = src_func_name[-1]\n",
    "    return src_func_name\n",
    "\n",
    "evaluate_func_names(\n",
    "    [parse_src_func_name(src) for src in batch_src],\n",
    "    decname=[camel_to_snake(extract_function_name(r.choices[0].message.content)) for r in decname_results],\n",
    "    ragname=[camel_to_snake(extract_function_name(r.choices[0].message.content)) for r in ragname_results],\n",
    "    parname=[camel_to_snake(extract_function_name(r.choices[0].message.content)) for r in parname_results]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
