{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Summarization\n",
    "\n",
    "This notebook contains experiments for binary summarization.\n",
    "\n",
    "## 1. Load Dataset\n",
    "\n",
    "We first load the test datasets. Here, `test_dataset` is for decompiled-only and retrieval-augmented inputs, while `probed_test_dataset` is for ProRec inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# load dataset\n",
    "CACHE_DIR = '../save/.cache'\n",
    "OUTPUT_DIR ='../save/binsum'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    'PurCL/lmpa-prorec-test-1k',\n",
    "    cache_dir=CACHE_DIR\n",
    ")['test']\n",
    "probed_test_dataset = load_dataset(\n",
    "    'PurCL/lmpa-prorec-test-probed-c34b-1k',\n",
    "    cache_dir=CACHE_DIR\n",
    ")['test']\n",
    "print(test_dataset)\n",
    "print(probed_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datastore\n",
    "\n",
    "The retrieval results are pre-computed and stored in the `test_dataset` in the form of datastore source function ids. Therefore, we need to access the datastore to map those ids to real functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from evaluator.binsum_evaluator import tokenizer\n",
    "from tools.retriever import CrossModalRetriever\n",
    "\n",
    "hf_hub_download(\n",
    "    repo_id='PurCL/casp-moco-lmpa-c-only',\n",
    "    subfolder='index',\n",
    "    filename='keys.pkl',\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "hf_hub_download(\n",
    "    repo_id='PurCL/casp-moco-lmpa-c-only',\n",
    "    subfolder='index',\n",
    "    filename='key_embeddings.npy',\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "index_path = '../save/.cache/models--PurCL--casp-moco-lmpa-c-only/snapshots/472d22fe7590403f9078799c6b83ff645eff9534/index'\n",
    "\n",
    "retriever = CrossModalRetriever()   # without model\n",
    "retriever.load_index(index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing\n",
    "\n",
    "We randomly sample 100 examples for reproduction. The pre-processing includes normalizing strings in the decompiled code and constructing inputs for three methods to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# sampled_indexes = random.sample(range(len(test_dataset)), 10)\n",
    "sampled_indexes = random.sample(range(len(test_dataset)), 100)\n",
    "\n",
    "\n",
    "string_pattern = r'([\\'\"])(?:(?=(\\\\?))\\2.)*?\\1'\n",
    "def normalize_string(raw_decompiled_function):\n",
    "    normalized_decompiled_function = re.sub(string_pattern, '<some_string>', raw_decompiled_function)\n",
    "    return normalized_decompiled_function\n",
    "\n",
    "\n",
    "def extract_probed(probed_sources):\n",
    "    strip_len = len('<asm_token>\\n')\n",
    "    processed_probed_sources = []\n",
    "    for ps in probed_sources:\n",
    "        asm_idx = ps.find('<asm_token>')\n",
    "        if asm_idx == -1:\n",
    "            processed_probed_sources.append(ps)\n",
    "        else:\n",
    "            pps = ps[asm_idx + strip_len:]\n",
    "            processed_probed_sources.append(pps)\n",
    "    return processed_probed_sources\n",
    "\n",
    "# TOP_K = 3\n",
    "# TOP_K = 4\n",
    "TOP_K = 5\n",
    "\n",
    "batch_lmpa = []\n",
    "batch_dec = []\n",
    "batch_dec_w_pro = []\n",
    "batch_src = []\n",
    "batch_dec_w_ret = []\n",
    "for ex_q, ex_p in zip(test_dataset.select(sampled_indexes), \\\n",
    "                probed_test_dataset.select(sampled_indexes)):\n",
    "    assert ex_q['src'] == ex_p['oracle_source']\n",
    "    batch_lmpa.append(ex_q['lmpa'])\n",
    "    decompiled_function = eval(ex_q['lmpa'])['query'].split('Q:[')[0].strip()\n",
    "    decompiled_function = normalize_string(decompiled_function)\n",
    "    batch_dec.append(decompiled_function)\n",
    "    batch_dec_w_pro.append(\n",
    "        (decompiled_function, extract_probed(ex_p['probed_sources'])[:TOP_K])\n",
    "    )\n",
    "    batch_src.append(ex_p['oracle_source'])\n",
    "    retrieved_sources = []\n",
    "    for score, idx in zip(ex_q['retrieved_index'][0], ex_q['retrieved_index'][1]):\n",
    "        ret_src = retriever.index['keys'][int(idx)]\n",
    "        retrieved_sources.append(tokenizer.decode(tokenizer.encode(ret_src)[:512]))\n",
    "    batch_dec_w_ret.append(\n",
    "        (decompiled_function, retrieved_sources[:TOP_K])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt GPT3.5\n",
    "\n",
    "First, construct prompter. Provide your own OpenAI token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.prompter import (\n",
    "    OpenAIDecompSummarizer, \n",
    "    OpenAISourceSummarizer, \n",
    ")\n",
    "\n",
    "# key\n",
    "secret_key = 'YOUR_OPENAI_KEY'\n",
    "\n",
    "MODEL_NAME = 'gpt-3.5-turbo-1106'\n",
    "MODEL_NAME_SHORT = 'gpt3.5'\n",
    "\n",
    "src_summarizer = OpenAISourceSummarizer(\n",
    "    secret_key,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "dec_summarizer = OpenAIDecompSummarizer(\n",
    "    secret_key,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "generation_config = {'temperature': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate Summarization for Source Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcsum_results = []\n",
    "for inputs in batch_src:\n",
    "    srcsum_results.append(src_summarizer.generate(inputs, **generation_config))\n",
    "with open(os.path.join(OUTPUT_DIR, f'srcsum_{MODEL_NAME_SHORT}_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(srcsum_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(srcsum_results[7].choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Summarization with Only Decompiled Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decsum_results = []\n",
    "for inputs in batch_dec:\n",
    "    decsum_results.append(dec_summarizer.generate(inputs, **generation_config))\n",
    "with open(os.path.join(OUTPUT_DIR, f'decsum_{MODEL_NAME_SHORT}_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(decsum_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generate Summaration with Retrieval-based Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragsum_results = []\n",
    "for inputs in batch_dec_w_ret:\n",
    "    ragsum_results.append(dec_summarizer.generate_with_augmentation(inputs, **generation_config))\n",
    "with open(os.path.join(OUTPUT_DIR, f'ragsum_{MODEL_NAME_SHORT}_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(ragsum_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Generate Summarization with ProRec Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsum_results = []\n",
    "for inputs in batch_dec_w_pro:\n",
    "    parsum_results.append(dec_summarizer.generate_with_augmentation(inputs, **generation_config))\n",
    "with open(os.path.join(OUTPUT_DIR, f'parsum_{MODEL_NAME_SHORT}_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(parsum_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation with CHRF and METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluator.binsum_evaluator\n",
    "reload(evaluator.binsum_evaluator)\n",
    "from evaluator.binsum_evaluator import (\n",
    "    extract_summary,\n",
    "    evaluate_automatic_metrics, \n",
    ")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, f'aggregated_{MODEL_NAME_SHORT}_results.json'), 'w') as f:\n",
    "    agg_results = []\n",
    "    for srcsum_res, decsum_res, parsum_res, ragsum_res, dw_pro, dw_ret, src, lmpa_data in \\\n",
    "        zip(srcsum_results, decsum_results, parsum_results, ragsum_results, batch_dec_w_pro, \n",
    "        batch_dec_w_ret, batch_src, batch_lmpa):\n",
    "        ref = extract_summary(srcsum_res.choices[0].message.content)\n",
    "        pred_dec = extract_summary(decsum_res.choices[0].message.content)\n",
    "        pred_pro = extract_summary(parsum_res.choices[0].message.content)\n",
    "        pred_rag = extract_summary(ragsum_res.choices[0].message.content)\n",
    "        agg_results.append(\n",
    "            {\n",
    "                'lmpa': lmpa_data,\n",
    "                'src': src,\n",
    "                'dec': dw_pro[0],\n",
    "                'srcsum': ref,\n",
    "                'decsum': pred_dec,\n",
    "                'prosum': pred_pro,\n",
    "                'probed_sources': dw_pro[1],\n",
    "                'ragsum': pred_rag,\n",
    "                'retrieved_sources': dw_ret[1],\n",
    "            }\n",
    "        )\n",
    "    print(len(agg_results))\n",
    "    json.dump(agg_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_automatic_metrics(\n",
    "# evaluate_detailed_automatic_metrics(\n",
    "    [extract_summary(r.choices[0].message.content) for r in srcsum_results],\n",
    "    decsum=[extract_summary(r.choices[0].message.content) for r in decsum_results],\n",
    "    ragsum=[extract_summary(r.choices[0].message.content) for r in ragsum_results],\n",
    "    parsum=[extract_summary(r.choices[0].message.content) for r in parsum_results],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
